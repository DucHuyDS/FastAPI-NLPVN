{"cells":[{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-01-23T09:45:48.048606Z","iopub.status.busy":"2023-01-23T09:45:48.047972Z","iopub.status.idle":"2023-01-23T09:45:58.294515Z","shell.execute_reply":"2023-01-23T09:45:58.293344Z","shell.execute_reply.started":"2023-01-23T09:45:48.048547Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install transformers \n","\n","# # !pip install wget"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-01-23T09:45:58.297835Z","iopub.status.busy":"2023-01-23T09:45:58.297419Z","iopub.status.idle":"2023-01-23T09:45:58.304588Z","shell.execute_reply":"2023-01-23T09:45:58.303644Z","shell.execute_reply.started":"2023-01-23T09:45:58.297793Z"},"trusted":true},"outputs":[],"source":["import torch\n","from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n","from transformers import AutoTokenizer,AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n","from transformers import Trainer, TrainingArguments, Seq2SeqTrainer, Seq2SeqTrainingArguments\n","import numpy as np\n","import random\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","import os \n","import re\n","os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-01-23T09:45:58.306982Z","iopub.status.busy":"2023-01-23T09:45:58.306263Z","iopub.status.idle":"2023-01-23T09:45:58.317514Z","shell.execute_reply":"2023-01-23T09:45:58.316589Z","shell.execute_reply.started":"2023-01-23T09:45:58.306935Z"},"trusted":true},"outputs":[],"source":["import re\n","def clean(data):\n","    with open('dataset/teencode.txt','r') as file:\n","      file = file.read()\n","      lines = file.split('\\n')\n","      for line in lines:\n","        elements = line.split('\\t')\n","        data = re.sub(r'\\b{}\\b'.format(elements[0]), elements[1], data)\n","    data = re.sub('per ','',data)\n","    data = re.sub(r'\\s+', ' ', data)\n","    return data\n","  # print(s)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-01-23T09:45:58.331776Z","iopub.status.busy":"2023-01-23T09:45:58.331104Z","iopub.status.idle":"2023-01-23T09:46:10.597969Z","shell.execute_reply":"2023-01-23T09:46:10.596936Z","shell.execute_reply.started":"2023-01-23T09:45:58.331739Z"},"trusted":true},"outputs":[],"source":["\n","# call the function\n","df_train = pd.read_excel('dataset/train.xlsx')\n","df_test =  pd.read_excel('dataset/test.xlsx')\n","df_valid = pd.read_excel('dataset/valid.xlsx')\n","\n","df_train['Sentence'] = df_train['Sentence'].apply(clean)\n","df_test['Sentence'] = df_test['Sentence'].apply(clean)\n","df_valid['Sentence'] = df_valid['Sentence'].apply(clean)\n","\n","\n","\n","train_texts = list(df_train['Sentence'])\n","test_texts = list(df_test['Sentence'])\n","valid_texts = list(df_valid['Sentence'])\n","\n","y= LabelEncoder()\n","\n","train_labels = y.fit_transform(df_train['Emotion'])\n","test_labels = y.fit_transform(df_test['Emotion'])\n","valid_labels = y.fit_transform(df_valid['Emotion'])\n","\n","target_names = list(df_train.Emotion.unique())\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-01-23T09:46:10.599639Z","iopub.status.busy":"2023-01-23T09:46:10.599272Z","iopub.status.idle":"2023-01-23T09:46:14.446365Z","shell.execute_reply":"2023-01-23T09:46:14.445380Z","shell.execute_reply.started":"2023-01-23T09:46:10.599588Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/vinai/phobert-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/44162aafdccde1e25f76f0d54580322fac02b26e8eacb597f24bd6bc187681fd.ef527b5c1bcf4a449629ee818cd749a4d27ce387432d22f53654850fbf98521f\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-large\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file https://huggingface.co/vinai/phobert-large/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/7a3db27d6661fa4783906394a2a4be66581cfedc8a8c85a3e6e4dbaaadd7ad46.26ba0c8945e559c68d0bc35d24fea16f5463a49fe8f134e0c32261d590b577fa\n","loading file https://huggingface.co/vinai/phobert-large/resolve/main/bpe.codes from cache at /root/.cache/huggingface/transformers/cbce691e0e23943f85d125308c08b413dd97987dfb57cd1ea4f27ba7f2b9f1e3.301ac8958de708ddcea8500d9acbe6261dba391d249c98dcda1e49dbbff870dd\n","loading file https://huggingface.co/vinai/phobert-large/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/vinai/phobert-large/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/vinai/phobert-large/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/vinai/phobert-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/44162aafdccde1e25f76f0d54580322fac02b26e8eacb597f24bd6bc187681fd.ef527b5c1bcf4a449629ee818cd749a4d27ce387432d22f53654850fbf98521f\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-large\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","Adding <mask> to the vocabulary\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["def set_seed(seed: int):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    if is_torch_available():\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        # safe to call this function even if cuda is not available\n","    if is_tf_available():\n","        import tensorflow as tf\n","        tf.random.set_seed(seed)\n","\n","set_seed(1)\n","\n","# the model we gonna train, base uncased BERT\n","# check text classification models here: https://huggingface.co/models?filter=text-classification\n","\n","model_name = \"vinai/phobert-large\" \n","\n","# max sequence length for each document/sentence sample\n","max_length = 512\n","# load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-01-23T09:46:14.448716Z","iopub.status.busy":"2023-01-23T09:46:14.448051Z","iopub.status.idle":"2023-01-23T09:46:29.092492Z","shell.execute_reply":"2023-01-23T09:46:29.091631Z","shell.execute_reply.started":"2023-01-23T09:46:14.448675Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/vinai/phobert-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/44162aafdccde1e25f76f0d54580322fac02b26e8eacb597f24bd6bc187681fd.ef527b5c1bcf4a449629ee818cd749a4d27ce387432d22f53654850fbf98521f\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-large\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"id2label\": {\n","    \"0\": \"Anger\",\n","    \"1\": \"Disgust\",\n","    \"2\": \"Enjoyment\",\n","    \"3\": \"Fear\",\n","    \"4\": \"Other\",\n","    \"5\": \"Sadness\",\n","    \"6\": \"Surprise\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"label2id\": {\n","    \"Anger\": 0,\n","    \"Disgust\": 1,\n","    \"Enjoyment\": 2,\n","    \"Fear\": 3,\n","    \"Other\": 4,\n","    \"Sadness\": 5,\n","    \"Surprise\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file https://huggingface.co/vinai/phobert-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/294a705e5145d401192e6317e93a0e9497682c465dffb05512ea04b30d26d12a.c4be549ad634ff94fe359dba58f778d17430acd3fdc403c5b338c4ba5087c684\n","Some weights of the model checkpoint at vinai/phobert-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# tokenize the dataset, truncate when passed `max_length`, \n","# and pad with 0's when less than `max_length`\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n","valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length)\n","\n","\n","# https://huggingface.co/transformers/v3.4.0/custom_datasets.html\n","class NewsGroupsDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n","        item[\"labels\"] = torch.tensor([self.labels[idx]])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# convert our tokenized data into a torch Dataset\n","\n","train_dataset = NewsGroupsDataset(train_encodings, train_labels)\n","valid_dataset = NewsGroupsDataset(valid_encodings, valid_labels)\n","test_dataset = NewsGroupsDataset(test_encodings, test_labels)\n","\n","label2id = {\"Anger\": 0, \"Disgust\": 1, \"Enjoyment\": 2, \"Fear\": 3, \"Other\": 4, \"Sadness\": 5, \"Surprise\": 6}\n","id2label = {0: \"Anger\", 1: \"Disgust\", 2: \"Enjoyment\", 3: \"Fear\", 4: \"Other\", 5: \"Sadness\", 6: \"Surprise\"}\n","\n","model = AutoModelForSequenceClassification.from_pretrained(model_name,label2id=label2id,\n","                        id2label=id2label, num_labels=7).to(\"cuda\")\n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-01-23T09:46:29.094465Z","iopub.status.busy":"2023-01-23T09:46:29.094101Z","iopub.status.idle":"2023-01-23T10:16:05.619861Z","shell.execute_reply":"2023-01-23T10:16:05.618937Z","shell.execute_reply.started":"2023-01-23T09:46:29.094426Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5548\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 15\n","  Total train batch size (w. parallel, distributed & accumulation) = 15\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1850\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1850' max='1850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1850/1850 29:29, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.515700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.007000</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.564100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to my_model/checkpoint-500\n","Configuration saved in my_model/checkpoint-500/config.json\n","Model weights saved in my_model/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in my_model/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in my_model/checkpoint-500/special_tokens_map.json\n","added tokens file saved in my_model/checkpoint-500/added_tokens.json\n","Saving model checkpoint to my_model/checkpoint-1000\n","Configuration saved in my_model/checkpoint-1000/config.json\n","Model weights saved in my_model/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in my_model/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in my_model/checkpoint-1000/special_tokens_map.json\n","added tokens file saved in my_model/checkpoint-1000/added_tokens.json\n","Saving model checkpoint to my_model/checkpoint-1500\n","Configuration saved in my_model/checkpoint-1500/config.json\n","Model weights saved in my_model/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in my_model/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in my_model/checkpoint-1500/special_tokens_map.json\n","added tokens file saved in my_model/checkpoint-1500/added_tokens.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 686\n","  Batch size = 15\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='93' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [46/46 00:11]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 1.4691859483718872,\n"," 'eval_accuracy': 0.6311953352769679,\n"," 'eval_runtime': 5.9762,\n"," 'eval_samples_per_second': 114.789,\n"," 'eval_steps_per_second': 7.697,\n"," 'epoch': 5.0}"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    # calculate accuracy using sklearn's function\n","    acc = accuracy_score(labels, preds)\n","    return {\n","      'accuracy': acc,\n","    }\n","\n","training_args = TrainingArguments(\n","    output_dir='my_model',          # output directory\n","    num_train_epochs=5,              # total number of training epochs\n","    per_device_train_batch_size=15,  # batch size per device during training\n","    per_device_eval_batch_size=15,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    learning_rate=4e-5,\n","\n","\n",")\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=valid_dataset,          # evaluation dataset\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",")\n","# train the model\n","trainer.train()\n","# evaluate the current model after training\n","trainer.evaluate()\n","# saving the fine tuned model & tokenizer\n","\n","# model_path = \"..output/vinai/phobert-base\"\n","# model.save_pretrained(model_path)\n","# tokenizer.save_pretrained(model_path)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-01-23T10:16:05.621419Z","iopub.status.busy":"2023-01-23T10:16:05.621082Z","iopub.status.idle":"2023-01-23T10:16:11.341595Z","shell.execute_reply":"2023-01-23T10:16:11.340765Z","shell.execute_reply.started":"2023-01-23T10:16:05.621384Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running Prediction *****\n","  Num examples = 693\n","  Batch size = 15\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.53      0.60      0.56        40\n","           1       0.66      0.59      0.62       132\n","           2       0.76      0.73      0.74       193\n","           3       0.67      0.76      0.71        46\n","           4       0.61      0.67      0.64       129\n","           5       0.69      0.72      0.70       116\n","           6       0.75      0.65      0.70        37\n","\n","    accuracy                           0.68       693\n","   macro avg       0.67      0.67      0.67       693\n","weighted avg       0.68      0.68      0.68       693\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","import numpy as np\n","pred = trainer.predict(test_dataset)\n","\n","# print(test_labels)\n","y_pred = np.argmax(pred.predictions, axis=1)\n","# print(y_pred)\n","print(classification_report(test_labels,y_pred))"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-01-23T10:18:40.990891Z","iopub.status.busy":"2023-01-23T10:18:40.990196Z","iopub.status.idle":"2023-01-23T10:18:44.026271Z","shell.execute_reply":"2023-01-23T10:18:44.025054Z","shell.execute_reply.started":"2023-01-23T10:18:40.990843Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to trainer-phobert\n","Configuration saved in trainer-phobert/config.json\n","Model weights saved in trainer-phobert/pytorch_model.bin\n","tokenizer config file saved in trainer-phobert/tokenizer_config.json\n","Special tokens file saved in trainer-phobert/special_tokens_map.json\n","added tokens file saved in trainer-phobert/added_tokens.json\n"]}],"source":["trainer.save_model('trainer-phobert')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"9218634a165858b2dd7cb4dffbc242499ee9c1091ce9f0509448aec65563d242"}}},"nbformat":4,"nbformat_minor":4}
